import torch
import torch.nn as nn
import torch.optim as optim
import time
import math
from src.dataset import create_dataset_and_loaders, PAD_IDX
from src.model import Encoder, Decoder, Seq2Seq
from src.utils import init_weights, count_parameters, epoch_time

# --- 1. C·∫•u h√¨nh Hyperparameters ---
BATCH_SIZE = 128
N_EPOCHS = 10           # S·ªë l·∫ßn h·ªçc l·∫∑p l·∫°i to√†n b·ªô d·ªØ li·ªáu
CLIP = 1                # C·∫Øt gradient ƒë·ªÉ tr√°nh b√πng n·ªï (ƒë·∫∑c tr∆∞ng c·ªßa LSTM)
LEARNING_RATE = 0.001

# C·∫•u h√¨nh Model (nh∆∞ trong ƒë·ªÅ b√†i g·ª£i √Ω)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

def train(model, iterator, optimizer, criterion, clip):
    model.train() # B·∫≠t ch·∫ø ƒë·ªô train (ƒë·ªÉ Dropout ho·∫°t ƒë·ªông)
    epoch_loss = 0
    
    for i, (src, trg) in enumerate(iterator):
        src, trg = src.to(device), trg.to(device)
        
        optimizer.zero_grad() # X√≥a s·∫°ch ƒë·∫°o h√†m c≈©
        
        # Forward pass
        output = model(src, trg)
        # output: [trg len, batch size, output dim]
        # trg: [trg len, batch size]
        
        # Reshape ƒë·ªÉ t√≠nh loss (b·ªè qua token ƒë·∫ßu ti√™n <sos>)
        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        
        # T√≠nh sai s·ªë
        loss = criterion(output, trg)
        
        # Backward pass (Lan truy·ªÅn ng∆∞·ª£c)
        loss.backward()
        
        # C·∫Øt gradient ƒë·ªÉ tr√°nh l·ªói exploding gradient
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        # C·∫≠p nh·∫≠t tr·ªçng s·ªë
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    model.eval() # B·∫≠t ch·∫ø ƒë·ªô ki·ªÉm tra (t·∫Øt Dropout)
    epoch_loss = 0
    
    with torch.no_grad(): # Kh√¥ng t√≠nh ƒë·∫°o h√†m cho nh·∫π m√°y
        for i, (src, trg) in enumerate(iterator):
            src, trg = src.to(device), trg.to(device)

            output = model(src, trg, teacher_forcing_ratio=0) # T·∫Øt Teacher Forcing khi test
            
            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            
            loss = criterion(output, trg)
            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

if __name__ == "__main__":
    # Ch·ªçn thi·∫øt b·ªã (∆∞u ti√™n GPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üöÄ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")

    # 1. Load d·ªØ li·ªáu
    print("‚è≥ ƒêang load d·ªØ li·ªáu...")
    train_loader, val_loader, test_loader, en_vocab, fr_vocab = create_dataset_and_loaders(BATCH_SIZE)
    
    INPUT_DIM = len(en_vocab)
    OUTPUT_DIM = len(fr_vocab)
    print(f"‚úÖ Vocab size: Anh={INPUT_DIM}, Ph√°p={OUTPUT_DIM}")

    # 2. Kh·ªüi t·∫°o Model
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)
    model = Seq2Seq(enc, dec, device).to(device)
    
    # Kh·ªüi t·∫°o tr·ªçng s·ªë & ƒë·∫øm tham s·ªë
    model.apply(init_weights)
    print(f"üìä M√¥ h√¨nh c√≥ {count_parameters(model):,} tham s·ªë c·∫ßn h·ªçc.")

    # 3. Optimizer & Loss
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # B·ªè qua token <pad> khi t√≠nh l·ªói

    # 4. V√≤ng l·∫∑p Training
    best_valid_loss = float('inf')
    
    print("\nüî• B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN üî•")
    for epoch in range(N_EPOCHS):
        start_time = time.time()
        
        train_loss = train(model, train_loader, optimizer, criterion, CLIP)
        valid_loss = evaluate(model, val_loader, criterion)
        
        end_time = time.time()
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        # N·∫øu loss gi·∫£m k·ª∑ l·ª•c th√¨ l∆∞u model l·∫°i
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), 'checkpoints/best_model.pth')
            saved_msg = "üíæ (ƒê√£ l∆∞u model t·ªët nh·∫•t)"
        else:
            saved_msg = ""
        
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} {saved_msg}')
        print(f'\tPPL: {math.exp(valid_loss):.3f}') # Perplexity (ch·ªâ s·ªë ƒë·ªô b·ªëi r·ªëi c·ªßa model)

    print("\nüéâ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T!")