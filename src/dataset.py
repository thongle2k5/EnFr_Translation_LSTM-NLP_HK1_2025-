import torch
import spacy
from collections import Counter
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset

# --- 1. C·∫•u h√¨nh Token & Spacy ---
# Load m√¥ h√¨nh ng√¥n ng·ªØ Spacy (ƒë√£ t·∫£i ·ªü b∆∞·ªõc tr∆∞·ªõc)
spacy_en = spacy.load("en_core_web_sm")
spacy_fr = spacy.load("fr_core_news_sm")

def tokenize_en(text):
    """T√°ch t·ª´ ti·∫øng Anh: "Hello world." -> ["Hello", "world", "."]"""
    return [tok.text for tok in spacy_en.tokenizer(text)]

def tokenize_fr(text):
    """T√°ch t·ª´ ti·∫øng Ph√°p"""
    return [tok.text for tok in spacy_fr.tokenizer(text)]

# C√°c ch·ªâ s·ªë ƒë·∫∑c bi·ªát
UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3
SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<sos>', '<eos>']

# --- 2. Class T·ª± X√¢y D·ª±ng T·ª´ ƒêi·ªÉn (Thay th·∫ø Torchtext) ---
class Vocab:
    def __init__(self, counter, min_freq=2):
        # Kh·ªüi t·∫°o map: Token -> ID (b·∫Øt ƒë·∫ßu b·∫±ng c√°c token ƒë·∫∑c bi·ªát)
        self.stoi = {tok: i for i, tok in enumerate(SPECIAL_SYMBOLS)}
        self.itos = {i: tok for i, tok in enumerate(SPECIAL_SYMBOLS)}
        idx = len(SPECIAL_SYMBOLS)
        
        # Duy·ªát qua c√°c t·ª´ ƒë·∫øm ƒë∆∞·ª£c, n·∫øu xu·∫•t hi·ªán ƒë·ªß nhi·ªÅu th√¨ th√™m v√†o t·ª´ ƒëi·ªÉn
        for word, count in counter.items():
            if count >= min_freq:
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1
                
    def __len__(self):
        return len(self.stoi)
        
    def __getitem__(self, token):
        # L·∫•y ID c·ªßa token, n·∫øu kh√¥ng c√≥ tr·∫£ v·ªÅ UNK_IDX
        return self.stoi.get(token, UNK_IDX)

def build_vocab_manual(filepath, tokenizer):
    """ƒê·ªçc file text v√† ƒë·∫øm t·∫ßn su·∫•t t·ª´"""
    counter = Counter()
    print(f"üìñ ƒêang qu√©t t·ª´ v·ª±ng file: {filepath}...")
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            tokens = tokenizer(line.strip())
            counter.update(tokens)
    return Vocab(counter, min_freq=2)

# --- 3. Dataset & Transform ---
class EnFrDataset(Dataset):
    def __init__(self, src_path, trg_path, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):
        self.data = []
        print(f"loading data {src_path}...")
        with open(src_path, 'r', encoding='utf-8') as f_src, open(trg_path, 'r', encoding='utf-8') as f_trg:
            for line_src, line_trg in zip(f_src, f_trg):
                # Tokenize v√† chuy·ªÉn sang ID ngay l√∫c load ƒë·ªÉ code g·ªçn
                src_tokens = [SOS_IDX] + [src_vocab[t] for t in src_tokenizer(line_src.strip())] + [EOS_IDX]
                trg_tokens = [SOS_IDX] + [trg_vocab[t] for t in trg_tokenizer(line_trg.strip())] + [EOS_IDX]
                self.data.append((torch.tensor(src_tokens), torch.tensor(trg_tokens)))
                
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    src_batch, trg_batch = [], []
    for src_item, trg_item in batch:
        src_batch.append(src_item)
        trg_batch.append(trg_item)
    
    # Padding
    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)
    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)
    return src_batch, trg_batch

# --- 4. H√†m Main g·ªçi t·ª´ b√™n ngo√†i ---
def create_dataset_and_loaders(batch_size=128):
    # B∆∞·ªõc 1: X√¢y d·ª±ng t·ª´ ƒëi·ªÉn th·ªß c√¥ng
    en_vocab = build_vocab_manual('data/raw/train.en', tokenize_en)
    fr_vocab = build_vocab_manual('data/raw/train.fr', tokenize_fr)
    
    print(f"‚úÖ ƒê√£ x√¢y xong Vocab! Anh: {len(en_vocab)}, Ph√°p: {len(fr_vocab)}")

    # B∆∞·ªõc 2: T·∫°o Dataset
    train_ds = EnFrDataset('data/raw/train.en', 'data/raw/train.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)
    val_ds = EnFrDataset('data/raw/val.en', 'data/raw/val.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)
    test_ds = EnFrDataset('data/raw/test.en', 'data/raw/test.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)

    # B∆∞·ªõc 3: DataLoader
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    return train_loader, val_loader, test_loader, en_vocab, fr_vocab