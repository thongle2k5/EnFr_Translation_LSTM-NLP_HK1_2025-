{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b607cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dc152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train_en = '../data/raw/train.en' \n",
    "filepath_train_fr = '../data/raw/train.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf9d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "    fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"ch∆∞a t·∫£i ƒë∆∞·ª£c g√≥i ng√¥n ng·ªØ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640584d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path, tokenizer):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6356b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "\n",
    "vocab_en = build_vocab_from_iterator(\n",
    "    yield_tokens(filepath_train_en, en_tokenizer),\n",
    "    min_freq=2,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=10000     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d92246",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en.set_default_index(vocab_en['<unk>'])\n",
    "\n",
    "vocab_fr = build_vocab_from_iterator(\n",
    "    yield_tokens(filepath_train_fr, fr_tokenizer),\n",
    "    min_freq=2,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=10000\n",
    ")\n",
    "vocab_fr.set_default_index(vocab_fr['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = vocab_en['<pad>']\n",
    "SOS_IDX = vocab_en['<sos>']\n",
    "EOS_IDX = vocab_en['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c22cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(tokenizer, vocab, text):\n",
    "    token_list = tokenizer(text.strip())\n",
    "    index_list = [vocab[token] for token in token_list]\n",
    "    return torch.tensor([SOS_IDX] + index_list + [EOS_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1c6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    src_lens = []\n",
    "\n",
    "    for src_sample, trg_sample in batch:\n",
    "        # Bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√¥ th√†nh tensor s·ªë\n",
    "        src_item = text_transform(en_tokenizer, vocab_en, src_sample)\n",
    "        trg_item = text_transform(fr_tokenizer, vocab_fr, trg_sample)\n",
    "        \n",
    "        src_batch.append(src_item)\n",
    "        trg_batch.append(trg_item)\n",
    "        # L∆∞u l·∫°i ƒë·ªô d√†i th·∫≠t c·ªßa c√¢u ti·∫øng Anh (ƒë·ªÉ d√πng cho pack_padded_sequence)\n",
    "        src_lens.append(len(src_item))\n",
    "\n",
    "    # --- B·∫ÆT BU·ªòC: S·∫Øp x·∫øp batch theo ƒë·ªô d√†i gi·∫£m d·∫ßn ---\n",
    "    #\n",
    "    # L√Ω do: PyTorch y√™u c·∫ßu input c·ªßa packing ph·∫£i ƒë∆∞·ª£c sort tr∆∞·ªõc\n",
    "    zipped = list(zip(src_batch, trg_batch, src_lens))\n",
    "    # S·∫Øp x·∫øp d·ª±a tr√™n src_lens (ph·∫ßn t·ª≠ th·ª© 2 trong tuple) t·ª´ cao xu·ªëng th·∫•p\n",
    "    zipped.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # T√°ch ng∆∞·ª£c tr·ªü l·∫°i th√†nh c√°c list ri√™ng l·∫ª\n",
    "    src_batch, trg_batch, src_lens = zip(*zipped)\n",
    "    \n",
    "    # Chuy·ªÉn src_lens sang tensor\n",
    "    src_lens = torch.tensor(src_lens)\n",
    "\n",
    "    # --- PADDING: ƒêi·ªÅn th√™m <pad> v√†o c√¢u ng·∫Øn ---\n",
    "    # padding_value=PAD_IDX: ƒêi·ªÅn s·ªë 1 v√†o ch·ªó tr·ªëng\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return src_batch, trg_batch, src_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf72a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  #\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu th√¥ t·ª´ file v√†o list (ƒë·ªÉ ƒë∆∞a v√†o DataLoader)\n",
    "def read_raw_data(path_en, path_fr):\n",
    "    with open(path_en, encoding='utf-8') as f_en, open(path_fr, encoding='utf-8') as f_fr:\n",
    "        return list(zip(f_en, f_fr))\n",
    "\n",
    "\n",
    "train_data = read_raw_data(filepath_train_en, filepath_train_fr)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True # N√™n x√°o tr·ªôn d·ªØ li·ªáu khi train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f08eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KI·ªÇM TRA DATALOADER (PADDING & PACKING) ===\n",
      "‚úÖ K√≠ch th∆∞·ªõc Source Batch: torch.Size([25, 64])\n",
      "   (D√†i nh·∫•t trong batch x Batch Size)\n",
      "‚úÖ K√≠ch th∆∞·ªõc Target Batch: torch.Size([34, 64])\n",
      "‚úÖ Danh s√°ch ƒë·ªô d√†i (ƒë√£ s·∫Øp x·∫øp gi·∫£m d·∫ßn ch∆∞a?):\n",
      "tensor([25, 25, 22, 22, 22, 21, 20, 19, 19, 18, 18, 18, 18, 17, 17, 17, 17, 17,\n",
      "        17, 16, 16, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 11,\n",
      "        11, 11, 11, 11, 11, 10, 10, 10, 10,  9])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== KI·ªÇM TRA DATALOADER (PADDING & PACKING) ===\")\n",
    "src, trg, src_len = next(iter(train_loader))\n",
    "\n",
    "print(f\"‚úÖ K√≠ch th∆∞·ªõc Source Batch: {src.shape}\")\n",
    "print(f\"   (D√†i nh·∫•t trong batch x Batch Size)\")\n",
    "print(f\"‚úÖ K√≠ch th∆∞·ªõc Target Batch: {trg.shape}\")\n",
    "print(f\"‚úÖ Danh s√°ch ƒë·ªô d√†i (ƒë√£ s·∫Øp x·∫øp gi·∫£m d·∫ßn ch∆∞a?):\")\n",
    "print(src_len) # In ra xem c√≥ ph·∫£i l√† 1 d√£y s·ªë gi·∫£m d·∫ßn kh√¥ng (VD: 20, 19, 15, 10...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e37b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # L∆∞u c√°c tham s·ªë\n",
    "        self.hid_dim = hid_dim      # K√≠ch th∆∞·ªõc tr·∫°ng th√°i ·∫©n (512)\n",
    "        self.n_layers = n_layers    # S·ªë l·ªõp LSTM (2)\n",
    "        \n",
    "        # 1. Embedding: Bi·∫øn index th√†nh vector dense\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # 2. Dropout: Tr√°nh overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 3. LSTM: X·ª≠ l√Ω chu·ªói\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        # src: [src len, batch size]\n",
    "        # src_len: [batch size] (ƒê·ªô d√†i th·∫≠t c·ªßa t·ª´ng c√¢u trong batch)\n",
    "        \n",
    "        # B∆∞·ªõc 1: Qua Embedding v√† Dropout\n",
    "        # embedded: [src len, batch size, emb dim]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # B∆∞·ªõc 2: PACKING (K·ªπ thu·∫≠t n√©n d·ªØ li·ªáu)\n",
    "        # Gi√∫p LSTM b·ªè qua c√°c s·ªë 0 (padding), ch·ªâ t√≠nh to√°n tr√™n t·ª´ th·∫≠t\n",
    "        #\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=True)\n",
    "        \n",
    "        # B∆∞·ªõc 3: ƒê∆∞a qua LSTM\n",
    "        # packed_outputs: ƒê·∫ßu ra d·∫°ng n√©n (ch·ª©a hidden state c·ªßa t·∫•t c·∫£ c√°c b∆∞·ªõc)\n",
    "        # hidden: Tr·∫°ng th√°i ·∫©n cu·ªëi c√πng (Context Vector) [n layers, batch size, hid dim]\n",
    "        # cell:   Tr·∫°ng th√°i t·∫ø b√†o nh·ªõ cu·ªëi c√πng (Context Vector)\n",
    "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # (T√πy ch·ªçn) N·∫øu mu·ªën l·∫•y output d·∫°ng th∆∞·ªùng th√¨ d√πng pad_packed_sequence\n",
    "        # outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        # Tr·∫£ v·ªÅ Context Vector ƒë·ªÉ Decoder s·ª≠ d·ª•ng\n",
    "        #\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3930f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ kh·ªüi t·∫°o Encoder:\n",
      "Encoder(\n",
      "  (embedding): Embedding(6191, 256)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      ")\n",
      "\n",
      "‚è≥ ƒêang test v·ªõi Batch Size = 64...\n",
      "   ƒê·ªô d√†i c√¢u d√†i nh·∫•t trong batch = 26\n",
      "\n",
      "=== K·∫æT QU·∫¢ TEST ENCODER ===\n",
      "‚úÖ Hidden State Shape: torch.Size([2, 64, 512])\n",
      "   (Mong ƒë·ª£i: [2, 64, 512]) -> [2, 64, 512]\n",
      "‚úÖ Cell State Shape:   torch.Size([2, 64, 512])\n",
      "   (Mong ƒë·ª£i: [2, 64, 512]) -> [2, 64, 512]\n",
      "\n",
      "üéâ CH√öC M·ª™NG! Encoder ho·∫°t ƒë·ªông ho√†n h·∫£o.\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH THAM S·ªê (Theo y√™u c·∫ßu ƒë·ªÅ b√†i) ---\n",
    "INPUT_DIM = len(vocab_en)  # K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Anh (Kho·∫£ng 6191)\n",
    "ENC_EMB_DIM = 256          #\n",
    "HID_DIM = 512              #\n",
    "N_LAYERS = 2               #\n",
    "ENC_DROPOUT = 0.5          #\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o Encoder:\")\n",
    "print(encoder)\n",
    "\n",
    "# --- CH·∫†Y TH·ª¨ V·ªöI 1 BATCH D·ªÆ LI·ªÜU ---\n",
    "# L·∫•y 1 batch t·ª´ DataLoader ƒë√£ l√†m ·ªü b∆∞·ªõc tr∆∞·ªõc\n",
    "src, trg, src_len = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\n‚è≥ ƒêang test v·ªõi Batch Size = {src.shape[1]}...\")\n",
    "print(f\"   ƒê·ªô d√†i c√¢u d√†i nh·∫•t trong batch = {src.shape[0]}\")\n",
    "\n",
    "# Ch·∫°y Forward\n",
    "# L∆∞u √Ω: Ph·∫£i truy·ªÅn c·∫£ src v√† src_len v√†o\n",
    "hidden, cell = encoder(src, src_len)\n",
    "\n",
    "print(\"\\n=== K·∫æT QU·∫¢ TEST ENCODER ===\")\n",
    "print(f\"‚úÖ Hidden State Shape: {hidden.shape}\")\n",
    "print(f\"   (Mong ƒë·ª£i: [{N_LAYERS}, {src.shape[1]}, {HID_DIM}]) -> [2, 64, 512]\")\n",
    "print(f\"‚úÖ Cell State Shape:   {cell.shape}\")\n",
    "print(f\"   (Mong ƒë·ª£i: [{N_LAYERS}, {src.shape[1]}, {HID_DIM}]) -> [2, 64, 512]\")\n",
    "\n",
    "if hidden.shape == (2, 64, 512):\n",
    "    print(\"\\nüéâ CH√öC M·ª™NG! Encoder ho·∫°t ƒë·ªông ho√†n h·∫£o.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è C√≥ g√¨ ƒë√≥ sai sai v·ªÅ k√≠ch th∆∞·ªõc output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1434e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # L∆∞u c√°c tham s·ªë\n",
    "        self.output_dim = output_dim # K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Ph√°p (kho·∫£ng 6555 t·ª´)\n",
    "        self.hid_dim = hid_dim       # 512\n",
    "        self.n_layers = n_layers     # 2\n",
    "        \n",
    "        # 1. Embedding\n",
    "        # Input l√† 1 t·ª´ ƒë∆°n l·∫ª -> vector\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # 2. LSTM\n",
    "        # Input size = emb_dim\n",
    "        # Hidden size = hid_dim\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        # 3. Linear (L·ªõp ƒë·∫ßu ra)\n",
    "        # Chuy·ªÉn t·ª´ tr·∫°ng th√°i ·∫©n (hid_dim) -> x√°c su·∫•t c·ªßa t·ª´ ti·∫øp theo (output_dim)\n",
    "        # C√¥ng th·ª©c: p(y_t) = softmax(Linear(h_t))\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        # 4. Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch size] (Ch·ªâ l√† 1 m·∫£ng ch·ª©a c√°c t·ª´ hi·ªán t·∫°i c·ªßa m·ªói c√¢u trong batch)\n",
    "        # hidden: [n layers, batch size, hid dim] (Context Vector t·ª´ b∆∞·ªõc tr∆∞·ªõc)\n",
    "        # cell:   [n layers, batch size, hid dim]\n",
    "        \n",
    "        # B∆∞·ªõc 1: Th√™m chi·ªÅu Seq Len v√†o input\n",
    "        # V√¨ LSTM y√™u c·∫ßu input shape [Seq Len, Batch, Dim] m√† ta ch·ªâ ch·∫°y 1 b∆∞·ªõc\n",
    "        # input: [1, batch size]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # B∆∞·ªõc 2: Embedding & Dropout\n",
    "        # embedded: [1, batch size, emb dim]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # B∆∞·ªõc 3: Cho qua LSTM\n",
    "        # output: [1, batch size, hid dim]\n",
    "        # hidden, cell: Tr·∫°ng th√°i m·ªõi ƒë·ªÉ d√πng cho b∆∞·ªõc sau\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        # B∆∞·ªõc 4: D·ª± ƒëo√°n t·ª´ ti·∫øp theo\n",
    "        # prediction: [batch size, output dim]\n",
    "        # Lo·∫°i b·ªè chi·ªÅu Seq Len b·∫±ng squeeze(0) tr∆∞·ªõc khi ƒë∆∞a v√†o Linear\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a68e9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ kh·ªüi t·∫°o Decoder:\n",
      "Decoder(\n",
      "  (embedding): Embedding(6555, 256)\n",
      "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "  (fc_out): Linear(in_features=512, out_features=6555, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "‚è≥ ƒêang test Decoder 1 b∆∞·ªõc...\n",
      "\n",
      "=== K·∫æT QU·∫¢ TEST DECODER ===\n",
      "‚úÖ Prediction Shape: torch.Size([64, 6555])\n",
      "   (Mong ƒë·ª£i: [Batch Size, Vocab Size]) -> [64, 6555]\n",
      "‚úÖ Hidden State Shape: torch.Size([2, 64, 512])\n",
      "   (Mong ƒë·ª£i: [Layers, Batch, Hid]) -> [2, 64, 512]\n",
      "\n",
      "üéâ CH√öC M·ª™NG! Decoder ho·∫°t ƒë·ªông ho√†n h·∫£o.\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH THAM S·ªê DECODER ---\n",
    "OUTPUT_DIM = len(vocab_fr) # K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Ph√°p (6555)\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Kh·ªüi t·∫°o Decoder\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o Decoder:\")\n",
    "print(decoder)\n",
    "\n",
    "# --- CH·∫†Y TH·ª¨ V·ªöI D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P ---\n",
    "# Gi·∫£ s·ª≠ ta ƒëang ·ªü b∆∞·ªõc ƒë·∫ßu ti√™n c·ªßa qu√° tr√¨nh d·ªãch\n",
    "# Input: L·∫•y ƒë·∫°i 1 batch ch·ª©a to√†n token <sos> (B·∫Øt ƒë·∫ßu c√¢u)\n",
    "input_test = torch.tensor([vocab_fr['<sos>']] * 64) # Batch size 64\n",
    "\n",
    "# Hidden, Cell: L·∫•y t·ª´ k·∫øt qu·∫£ test Encoder l√∫c n√£y (Context Vector)\n",
    "# (L∆∞u √Ω: Ph·∫£i ch·∫°y block test Encoder ·ªü tr√™n tr∆∞·ªõc nh√©)\n",
    "\n",
    "print(f\"\\n‚è≥ ƒêang test Decoder 1 b∆∞·ªõc...\")\n",
    "prediction, hidden_new, cell_new = decoder(input_test, hidden, cell)\n",
    "\n",
    "print(\"\\n=== K·∫æT QU·∫¢ TEST DECODER ===\")\n",
    "print(f\"‚úÖ Prediction Shape: {prediction.shape}\")\n",
    "print(f\"   (Mong ƒë·ª£i: [Batch Size, Vocab Size]) -> [64, {OUTPUT_DIM}]\")\n",
    "\n",
    "print(f\"‚úÖ Hidden State Shape: {hidden_new.shape}\")\n",
    "print(f\"   (Mong ƒë·ª£i: [Layers, Batch, Hid]) -> [2, 64, 512]\")\n",
    "\n",
    "if prediction.shape == (64, OUTPUT_DIM) and hidden_new.shape == (2, 64, 512):\n",
    "    print(\"\\nüéâ CH√öC M·ª™NG! Decoder ho·∫°t ƒë·ªông ho√†n h·∫£o.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è K√≠ch th∆∞·ªõc ƒë·∫ßu ra ch∆∞a ƒë√∫ng, ki·ªÉm tra l·∫°i code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7c262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # Ki·ªÉm tra k·ªπ thu·∫≠t: K√≠ch th∆∞·ªõc hidden c·ªßa Encoder v√† Decoder ph·∫£i kh·ªõp nhau\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, src_len, teacher_forcing_ratio=0.5):\n",
    "        # src: [src len, batch size]\n",
    "        # trg: [trg len, batch size]\n",
    "        # src_len: [batch size] (ƒê·ªô d√†i th·∫≠t c·ªßa c√¢u ngu·ªìn)\n",
    "        # teacher_forcing_ratio: X√°c su·∫•t d√πng t·ª´ th·∫≠t ƒë·ªÉ train (m·∫∑c ƒë·ªãnh 0.5)\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor ƒë·ªÉ l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n (ban ƒë·∫ßu to√†n s·ªë 0)\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # B∆∞·ªõc 1: ƒê∆∞a c·∫£ c√¢u qua Encoder ƒë·ªÉ l·∫•y Context Vector (hidden, cell)\n",
    "        hidden, cell = self.encoder(src, src_len)\n",
    "        \n",
    "        # B∆∞·ªõc 2: B·∫Øt ƒë·∫ßu gi·∫£i m√£ (Decoder)\n",
    "        # Token ƒë·∫ßu ti√™n ƒë∆∞a v√†o Decoder lu√¥n l√† <sos> (Start of Sentence)\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        # V√≤ng l·∫∑p ch·∫°y t·ª´ng t·ª´ m·ªôt: t·ª´ 1 ƒë·∫øn h·∫øt c√¢u\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # Ch·∫°y Decoder 1 b∆∞·ªõc\n",
    "            # input: t·ª´ hi·ªán t·∫°i\n",
    "            # hidden, cell: tr·∫°ng th√°i c≈© -> c·∫≠p nh·∫≠t th√†nh m·ªõi\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o tensor outputs\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # --- TEACHER FORCING LOGIC ---\n",
    "            # Quy·∫øt ƒë·ªãnh xem c√≥ \"nh·∫Øc b√†i\" kh√¥ng?\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # L·∫•y t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t m√† m√°y v·ª´a ƒëo√°n ƒë∆∞·ª£c\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            # N·∫øu teacher_force = True -> l·∫•y t·ª´ ƒë√∫ng trong t·∫≠p trg (target)\n",
    "            # N·∫øu False -> l·∫•y t·ª´ m√°y v·ª´a ƒëo√°n (top1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f89875d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n",
      "‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng Seq2Seq Model!\n",
      "\n",
      "‚è≥ ƒêang ch·∫°y th·ª≠ Forward Pass...\n",
      "\n",
      "=== K·∫æT QU·∫¢ KI·ªÇM TRA SEQ2SEQ ===\n",
      "‚úÖ Output Shape: torch.Size([33, 64, 6555])\n",
      "   (Mong ƒë·ª£i: [Trg Len, Batch Size, Vocab Size]) -> [33, 64, 6555]\n",
      "\n",
      "üéâ TUY·ªÜT V·ªúI! M√¥ h√¨nh ƒë√£ s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. C·∫§U H√åNH THI·∫æT B·ªä ---\n",
    "# N·∫øu c√≥ GPU th√¨ d√πng, kh√¥ng th√¨ d√πng CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "# --- 2. KH·ªûI T·∫†O C√ÅC M√î H√åNH ---\n",
    "# Encoder v√† Decoder (ƒë√£ khai b√°o ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc)\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# M√¥ h√¨nh t·ªïng Seq2Seq\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng Seq2Seq Model!\")\n",
    "\n",
    "# --- 3. KH·ªûI T·∫†O TR·ªåNG S·ªê (T√πy ch·ªçn nh∆∞ng n√™n l√†m) ---\n",
    "# Gi√∫p model h·ªçc nhanh h∆°n b·∫±ng c√°ch kh·ªüi t·∫°o tham s·ªë ng·∫´u nhi√™n theo chu·∫©n\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "# --- 4. CH·∫†Y TH·ª¨ V·ªöI 1 BATCH ---\n",
    "# L·∫•y 1 batch d·ªØ li·ªáu\n",
    "src, trg, src_len = next(iter(train_loader))\n",
    "\n",
    "# Chuy·ªÉn d·ªØ li·ªáu sang thi·∫øt b·ªã (CPU/GPU)\n",
    "src = src.to(device)\n",
    "trg = trg.to(device)\n",
    "# src_len kh√¥ng c·∫ßn .to(device) v√¨ pack_padded_sequence c·∫ßn n√≥ ·ªü CPU\n",
    "\n",
    "print(f\"\\n‚è≥ ƒêang ch·∫°y th·ª≠ Forward Pass...\")\n",
    "# model(src, trg, src_len)\n",
    "output = model(src, trg, src_len)\n",
    "\n",
    "print(\"\\n=== K·∫æT QU·∫¢ KI·ªÇM TRA SEQ2SEQ ===\")\n",
    "print(f\"‚úÖ Output Shape: {output.shape}\")\n",
    "print(f\"   (Mong ƒë·ª£i: [Trg Len, Batch Size, Vocab Size]) -> [{trg.shape[0]}, 64, {OUTPUT_DIM}]\")\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ l·ªói chi·ªÅu kh√¥ng\n",
    "if output.shape == (trg.shape[0], 64, OUTPUT_DIM):\n",
    "    print(\"\\nüéâ TUY·ªÜT V·ªúI! M√¥ h√¨nh ƒë√£ s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è K√≠ch th∆∞·ªõc ƒë·∫ßu ra ch∆∞a ƒë√∫ng.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
